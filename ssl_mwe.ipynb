{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6c7a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.get_cells().map( function(c) {  return c.code_mirror.options.cursorBlinkRate=0;  } );\n",
       "CodeMirror.defaults.cursorBlinkRate=0;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.get_cells().map( function(c) {  return c.code_mirror.options.cursorBlinkRate=0;  } );\n",
    "CodeMirror.defaults.cursorBlinkRate=0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad678a",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Example\n",
    "\n",
    "This is a small scale example for self-supervised learning (SSL) in computer vision, using the MNIST data set (classification).\n",
    "\n",
    "The SSL approach follows https://arxiv.org/abs/2105.04906 (VICReg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa5596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device :  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_fct\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.transforms as tv_transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"using device : \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ef1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['figure.facecolor']    = 'White'\n",
    "plt.rcParams['font.size']    = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948f2dc",
   "metadata": {},
   "source": [
    "## Get all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0263294d",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "transform = tv_transforms.Compose([ tv_transforms.ToTensor(),\n",
    "                                    tv_transforms.Normalize((0.13066047797803165), (0.308107814120828)) ])\n",
    "transform_rnd_crop = tv_transforms.Compose([ tv_transforms.ToTensor(),\n",
    "                                             tv_transforms.Normalize((0.13066047797803165), (0.308107814120828)),\n",
    "                                             tv_transforms.RandomCrop( size = ( 14, 14 ) ) ])\n",
    "\n",
    "mnist_train                   = tv_datasets.MNIST('./cs231n/datasets', train=True, download=True,\n",
    "                                       transform=transform)\n",
    "mnist_train_rnd_crop_1        = tv_datasets.MNIST( './cs231n/datasets', train=True, download=True,\n",
    "                                                      transform = transform_rnd_crop )\n",
    "mnist_train_rnd_crop_2        = tv_datasets.MNIST( './cs231n/datasets', train=True, download=True,\n",
    "                                                      transform = transform_rnd_crop )\n",
    "loader_train_mnist            = DataLoader( mnist_train, batch_size=500, \n",
    "                                            sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)) )\n",
    "loader_train_mnist_rnd_crop_1 = DataLoader( mnist_train_rnd_crop_1, batch_size=500,\n",
    "                                            sampler = sampler.SubsetRandomSampler(range(NUM_TRAIN)) )\n",
    "loader_train_mnist_rnd_crop_2 = DataLoader( mnist_train_rnd_crop_2, batch_size=500,\n",
    "                                            sampler = sampler.SubsetRandomSampler(range(NUM_TRAIN)) )\n",
    "\n",
    "mnist_val = tv_datasets.MNIST('./cs231n/datasets', train=True, download=True,\n",
    "                               transform=transform)\n",
    "loader_val_mnist = DataLoader(mnist_val, batch_size=500,\n",
    "                              sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "mnist_test = tv_datasets.MNIST('./cs231n/datasets', train=False, download=True, \n",
    "                               transform=transform)\n",
    "loader_test_mnist = DataLoader(mnist_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e564c87",
   "metadata": {},
   "source": [
    "## Linear classifier as the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baba10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ll = nn.Linear( input_size, output_size )\n",
    "        nn.init.kaiming_normal_(self.ll.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view( x.shape[0], -1 )\n",
    "        scores = self.ll(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3004b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dl):\n",
    "    if dl.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dl:\n",
    "            x               = x.to(device=device, dtype=torch.float32)\n",
    "            y               = y.to(device=device, dtype=torch.int64)\n",
    "            scores          = model(x)\n",
    "            _, predictions  = scores.max(1)\n",
    "            num_correct    += ( predictions == y ).sum().item()\n",
    "            num_samples    += predictions.size(0)\n",
    "        accuracy = float( num_correct ) / float( num_samples )\n",
    "        print( \"Accuracy = {} %\".format( int( ( num_correct / num_samples ) * 100 ) ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39468e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dl_train, dl_val, n_epochs = 1):\n",
    "    model = model.to(device)\n",
    "    for i_epoch in range( n_epochs ):\n",
    "        for t, (x, y) in enumerate( dl_train ):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=torch.float32)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model( x )\n",
    "            loss   = torch_fct.cross_entropy( scores, y )\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch\", i_epoch, \":\")\n",
    "        get_accuracy( model, dl_val )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abda4a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 39 %\n",
      "\n",
      "epoch 1 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 57 %\n",
      "\n",
      "epoch 2 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 64 %\n",
      "\n",
      "epoch 3 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 68 %\n",
      "\n",
      "epoch 4 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 71 %\n",
      "\n",
      "epoch 5 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 73 %\n",
      "\n",
      "epoch 6 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 74 %\n",
      "\n",
      "epoch 7 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 75 %\n",
      "\n",
      "epoch 8 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 76 %\n",
      "\n",
      "epoch 9 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 78 %\n",
      "\n",
      "epoch 10 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 79 %\n",
      "\n",
      "epoch 11 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 79 %\n",
      "\n",
      "epoch 12 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 80 %\n",
      "\n",
      "epoch 13 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 80 %\n",
      "\n",
      "epoch 14 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 81 %\n",
      "\n",
      "epoch 15 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 81 %\n",
      "\n",
      "epoch 16 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 17 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 18 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 19 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 20 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 21 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 22 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 82 %\n",
      "\n",
      "epoch 23 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 83 %\n",
      "\n",
      "epoch 24 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 83 %\n",
      "\n",
      "epoch 25 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 83 %\n",
      "\n",
      "epoch 26 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 83 %\n",
      "\n",
      "epoch 27 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 84 %\n",
      "\n",
      "epoch 28 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 84 %\n",
      "\n",
      "epoch 29 :\n",
      "Checking accuracy on validation set\n",
      "Accuracy = 84 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel( 28 * 28, 10 )\n",
    "\n",
    "x, y = loader_train_mnist.dataset[0]\n",
    "\n",
    "optimizer = optim.SGD( model.parameters(), lr = 3e-4, momentum = 0.8 )\n",
    "train(model, optimizer, loader_train_mnist, loader_val_mnist, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8b739",
   "metadata": {},
   "source": [
    " # SSL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606373e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create a dataset suitable for SSL, using two datasets which represent different views of the same image.\n",
    "    For each entry, both datasets must contain the same label.\n",
    "    \"\"\"\n",
    "    def __init__( self, ds1, ds2 ):\n",
    "        assert len(ds1) == len(ds2)\n",
    "        assert ds1[0][0].dtype == ds2[0][0].dtype\n",
    "        assert type( ds1[0][1] ) == type( ds2[0][1] )\n",
    "        for i_entry in range( len( ds1 ) ):\n",
    "            _, y_1 =  ds1[i_entry]\n",
    "            _, y_2 =  ds2[i_entry]\n",
    "            assert y_1 == y_2\n",
    "        self.ds1 = ds1\n",
    "        self.ds2 = ds2\n",
    "        \n",
    "    def __len__( self ):\n",
    "        return len( self.ds1 )\n",
    "    \n",
    "    def __getitem__( self, idx ):\n",
    "        x_1, y  = self.ds1[idx]\n",
    "        x_2, _  = self.ds2[idx]\n",
    "        return ( x_1, x_2 ), y\n",
    "\n",
    "ds_ssl = SSLDataset( mnist_train_rnd_crop_1, mnist_train_rnd_crop_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713265d0",
   "metadata": {},
   "source": [
    "## SSL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31501b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLNet(nn.Module):\n",
    "    def __init__( self, n_channels_in, n_layers_backbone_last, n_nodes_expander ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Assuming a 14 x 14 input image\n",
    "        self.bb_conv_1    = nn.Conv2d( n_channels_in, 8, 5, padding = 2 )\n",
    "        nn.init.kaiming_normal_( self.bb_conv_1.weight )\n",
    "        self.bb_bn_1      = nn.BatchNorm2d( 8 )\n",
    "        self.bb_maxpool_1 = nn.MaxPool2d( 2 )\n",
    "        self.bb_conv_2    = nn.Conv2d( 8, n_layers_backbone_last, 4, padding = 1 )\n",
    "        nn.init.kaiming_normal_( self.bb_conv_2.weight )\n",
    "        self.bb_bn_2      = nn.BatchNorm2d( n_layers_backbone_last )\n",
    "        self.bb_maxpool_2 = nn.MaxPool2d( 2 )\n",
    "        \n",
    "        self.exp_fc_1 = nn.Linear( n_layers_backbone_last * 3**2, n_nodes_expander )\n",
    "        nn.init.kaiming_normal_( self.exp_fc_1.weight )\n",
    "        self.exp_bn_1 = nn.BatchNorm1d( n_nodes_expander )\n",
    "        self.exp_fc_2 = nn.Linear( n_nodes_expander, n_nodes_expander )\n",
    "        nn.init.kaiming_normal_( self.exp_fc_2.weight )\n",
    "        \n",
    "    def forward_backbone(self, x):\n",
    "        x = torch_fct.relu( self.bb_bn_1( self.bb_conv_1( x ) ) )\n",
    "        x = self.bb_maxpool_1( x )\n",
    "        x = torch_fct.relu( self.bb_bn_2( self.bb_conv_2( x ) ) )\n",
    "        x = self.bb_maxpool_2( x )\n",
    "        y = x.view(x.shape[0], -1)\n",
    "        return y\n",
    "        \n",
    "    def forward_expander(self, y):\n",
    "        y = torch_fct.relu( self.exp_bn_1( self.exp_fc_1( y ) ) )\n",
    "        z = self.exp_fc_2( y )\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.forward_backbone( x )\n",
    "        z = self.forward_expander( y )\n",
    "        return z\n",
    "\n",
    "ssl_model = SSLNet( 1, 12, 250 )\n",
    "dl_ssl    = DataLoader( ds_ssl, batch_size = 500, sampler=sampler.SequentialSampler(range(30000)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0dd2c",
   "metadata": {},
   "source": [
    "## SSL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "979ccdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl( model, optimizer, data_loader, n_epochs = 1 ):\n",
    "    \n",
    "    model        = model.to(device)\n",
    "    loss_fct_mse = nn.MSELoss()\n",
    "    \n",
    "    loss_mse_all = []\n",
    "    loss_std_all = []\n",
    "    loss_cov_all = []\n",
    "    loss_sum_all = []\n",
    "    \n",
    "    for i_epoch in range( n_epochs ):\n",
    "        for t, (XX, _) in enumerate( data_loader ):\n",
    "            \n",
    "            model.train()\n",
    "            x_1 = XX[0].to( device, dtype = torch.float32 )\n",
    "            x_2 = XX[1].to( device, dtype = torch.float32 )\n",
    "            z_1 = model( x_1 )\n",
    "            z_2 = model( x_2 )\n",
    "            \n",
    "            loss_mse = loss_fct_mse( z_1, z_2 )\n",
    "            loss_mse_all.append( loss_mse.detach().cpu() )\n",
    "            \n",
    "            std_z_1   = torch.sqrt( z_1.var( dim = 0 ) + 1e-4 )\n",
    "            std_z_2   = torch.sqrt( z_2.var( dim = 0 ) + 1e-4 )\n",
    "            loss_std  = torch.mean( torch_fct.relu( 1.0 - std_z_1 ) ) + torch.mean( torch_fct.relu( 1.0 - std_z_2 ) )\n",
    "            loss_std_all.append( loss_std.detach().cpu() )\n",
    "            \n",
    "            z_1        = z_1 - z_1.mean( dim = 0 )\n",
    "            z_2        = z_2 - z_2.mean( dim = 0 )\n",
    "            batch_size = z_1.shape[0]\n",
    "            exp_size   = z_1.shape[1]\n",
    "            cov_1      = z_1.T @ z_1 / ( batch_size - 1 )\n",
    "            cov_2      = z_2.T @ z_2 / ( batch_size - 1 )\n",
    "            loss_cov = 0.0\n",
    "            for i_row in range(1, exp_size - 1):\n",
    "                for i_column in range(i_row + 1, exp_size):\n",
    "                    loss_cov += 2.0 * ( cov_1[i_row][i_column]**2 + cov_2[i_row][i_column]**2 ) / exp_size\n",
    "            loss_cov_all.append( loss_cov.detach().cpu() )\n",
    "            \n",
    "            loss_sum = 25.0 * loss_mse + 25.0 * loss_std + 1.0 * loss_cov\n",
    "            loss_sum_all.append( loss_sum.detach().cpu() )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % 1 == 0:\n",
    "                print(\"Epoch {}, iteration {} : loss mse, std, cov = {}, {}, {}\".format( i_epoch, t * batch_size, loss_mse_all[-1], loss_std_all[-1], loss_cov_all[-1] ) )\n",
    "    \n",
    "    return { \"sum\" : loss_sum_all,\n",
    "             \"mse\" : loss_mse_all,\n",
    "             \"std\" : loss_std_all,\n",
    "             \"cov\" : loss_cov_all }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0 : loss mse, std, cov = 1.263320803642273, 0.27022677659988403, 4.547415256500244\n",
      "Epoch 0, iteration 500 : loss mse, std, cov = 1.2050282955169678, 0.3159358501434326, 3.872286558151245\n",
      "Epoch 0, iteration 1000 : loss mse, std, cov = 1.1330249309539795, 0.35493987798690796, 3.9316139221191406\n",
      "Epoch 0, iteration 1500 : loss mse, std, cov = 1.1037119626998901, 0.38783514499664307, 3.1407806873321533\n",
      "Epoch 0, iteration 2000 : loss mse, std, cov = 1.0636707544326782, 0.4195724129676819, 2.69088077545166\n",
      "Epoch 0, iteration 2500 : loss mse, std, cov = 1.0431727170944214, 0.44917625188827515, 2.4916329383850098\n",
      "Epoch 0, iteration 3000 : loss mse, std, cov = 1.0230759382247925, 0.47226446866989136, 2.203735589981079\n",
      "Epoch 0, iteration 3500 : loss mse, std, cov = 0.9762941598892212, 0.5044240951538086, 1.9838652610778809\n",
      "Epoch 0, iteration 4000 : loss mse, std, cov = 0.9250277280807495, 0.5363767743110657, 1.8295432329177856\n",
      "Epoch 0, iteration 4500 : loss mse, std, cov = 0.8824102878570557, 0.5660547614097595, 1.7655335664749146\n",
      "Epoch 0, iteration 5000 : loss mse, std, cov = 0.8414413928985596, 0.5957361459732056, 1.6023943424224854\n",
      "Epoch 0, iteration 5500 : loss mse, std, cov = 0.8119500875473022, 0.6169750094413757, 1.6035337448120117\n",
      "Epoch 0, iteration 6000 : loss mse, std, cov = 0.7641223669052124, 0.6581517457962036, 1.429897427558899\n",
      "Epoch 0, iteration 6500 : loss mse, std, cov = 0.773413896560669, 0.6571516990661621, 1.3297206163406372\n",
      "Epoch 0, iteration 7000 : loss mse, std, cov = 0.7650234699249268, 0.671729326248169, 1.2837885618209839\n",
      "Epoch 0, iteration 7500 : loss mse, std, cov = 0.7193057537078857, 0.6980631351470947, 1.2484099864959717\n",
      "Epoch 0, iteration 8000 : loss mse, std, cov = 0.6732121706008911, 0.7199805974960327, 1.3145911693572998\n",
      "Epoch 0, iteration 8500 : loss mse, std, cov = 0.6743753552436829, 0.7405516505241394, 1.236318588256836\n",
      "Epoch 0, iteration 9000 : loss mse, std, cov = 0.6671518683433533, 0.7404672503471375, 1.2198357582092285\n",
      "Epoch 0, iteration 9500 : loss mse, std, cov = 0.6527311205863953, 0.748637318611145, 1.1951498985290527\n",
      "Epoch 0, iteration 10000 : loss mse, std, cov = 0.5779692530632019, 0.7791076898574829, 1.5675286054611206\n",
      "Epoch 0, iteration 10500 : loss mse, std, cov = 0.5984246134757996, 0.7811806201934814, 1.3248140811920166\n",
      "Epoch 0, iteration 11000 : loss mse, std, cov = 0.5947034955024719, 0.7960852384567261, 1.239778757095337\n",
      "Epoch 0, iteration 11500 : loss mse, std, cov = 0.5822800993919373, 0.8118484020233154, 1.198441982269287\n",
      "Epoch 0, iteration 12000 : loss mse, std, cov = 0.5585382580757141, 0.818912148475647, 1.3343837261199951\n",
      "Epoch 0, iteration 12500 : loss mse, std, cov = 0.5490829944610596, 0.8178117871284485, 1.2934809923171997\n",
      "Epoch 0, iteration 13000 : loss mse, std, cov = 0.5305144190788269, 0.8420917987823486, 1.2808188199996948\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam( ssl_model.parameters(), lr = 1e-3, weight_decay = 1e-2 )\n",
    "losses    = train_ssl( ssl_model, optimizer, dl_ssl, 1 )\n",
    "\n",
    "loss_sum_all = losses[ \"sum\" ]\n",
    "loss_mse_all = losses[ \"mse\" ]\n",
    "loss_std_all = losses[ \"std\" ]\n",
    "loss_cov_all = losses[ \"cov\" ]\n",
    "\n",
    "plt.plot( [ x / max( loss_mse_all ) for x in loss_mse_all ], color = \"r\", label = \"MSE\" )\n",
    "plt.plot( [ x / max( loss_std_all ) for x in loss_std_all ], color = \"g\", label = \"STD\" )\n",
    "plt.plot( [ x / max( loss_cov_all ) for x in loss_cov_all ], color = \"b\", label = \"COV\" )\n",
    "plt.plot( [ x / max( loss_sum_all ) for x in loss_sum_all ], color = \"k\", label = \"SUM\" )\n",
    "plt.ylabel( \"Loss / Max Loss\" )\n",
    "plt.xlabel( \"Batch\" )\n",
    "plt.yscale( \"log\" )\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e97f2",
   "metadata": {},
   "source": [
    "## Downstream task: SSL pre-trained model + one linear layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43406a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLBasedLinearModel(nn.Module):\n",
    "    def __init__(self, ssl_pretrained_model, n_classes ):\n",
    "        super().__init__()\n",
    "        self.ssl_pretrained_model = ssl_pretrained_model\n",
    "        self.fc_layer             = nn.Linear( 432, n_classes )\n",
    "        nn.init.kaiming_normal_(self.fc_layer.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y      = self.ssl_pretrained_model.forward_backbone( x ).detach()\n",
    "        scores = self.fc_layer( y )\n",
    "        return scores\n",
    "\n",
    "downstream_model = SSLBasedLinearModel( ssl_model, 10 )\n",
    "downstream_model = downstream_model.to(device)\n",
    "x, _             = next( iter ( loader_train_mnist ) )\n",
    "x                = x.to(device)\n",
    "downstream_model( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD( downstream_model.parameters(), lr = 1e-3, momentum = 0.8 )\n",
    "train( downstream_model, optimizer, loader_train_mnist, loader_val_mnist, 10 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
